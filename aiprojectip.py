# -*- coding: utf-8 -*-
"""AIprojectIP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STHXH6bvuo6598G0tT8i52AldHfIyivx
"""

from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print(f'User uploaded file "{fn}" with length {len(uploaded[fn])} bytes')

!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000

from zipfile import ZipFile
file_name = "/content/skin-cancer-mnist-ham10000.zip"
with ZipFile(file_name, 'r') as zip:
    zip.extractall()
    print('Dataset extracted successfully')

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16  # Import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf

img_width, img_height = 128, 128
batch_size = 32
epochs = 20
metadata = pd.read_csv('/content/HAM10000_metadata.csv')
image_dirs = [
    '/content/HAM10000_images_part_1',
    '/content/HAM10000_images_part_2',
    '/content/ham10000_images_part_1',
    '/content/ham10000_images_part_2'
]

metadata.head()

image_paths = [os.path.join(image_dir, fname + '.jpg') for image_dir in image_dirs for fname in metadata['image_id']]
labels = metadata['dx'].values
label_to_index = {label: i for i, label in enumerate(np.unique(labels))}
labels = [label_to_index[label] for label in labels]

# print("Length of image_paths:", len(image_paths))
# print("Length of labels:", len(labels))

if len(image_paths) != len(labels):
    if len(image_paths) > len(labels):
        labels = labels + [labels[-1]] * (len(image_paths) - len(labels))
    else:
        image_paths = image_paths + [image_paths[-1]] * (len(labels) - len(image_paths))

# Split the data into train, validation, and test sets
train_data, test_data = train_test_split(pd.DataFrame({'image_path': image_paths, 'label': labels}), test_size=0.2, random_state=42)
train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)

# Data generators
train_datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    rescale=1./255
)

val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_dataframe(
    train_data,
    x_col='image_path',
    y_col='label',
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='raw'
)

val_generator = val_datagen.flow_from_dataframe(
    val_data,
    x_col='image_path',
    y_col='label',
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='raw'
)

test_generator = test_datagen.flow_from_dataframe(
    test_data,
    x_col='image_path',
    y_col='label',
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='raw'
)

# Base VGG16 model
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))

# Fine-tune the last few layers
for layer in base_model.layers:
    layer.trainable = True

# Model architecture
x = base_model.output
x = Flatten()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(512, activation='relu')(x)
x = BatchNormalization()(x)
output = Dense(len(label_to_index), activation='softmax')(x)  # Output layer for multi-class classification

model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Callbacks
best_model = ModelCheckpoint('best_model.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)  # Increased patience
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)  # Adjusted patience

history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=val_generator,
    callbacks=[best_model, early_stop, reduce_lr]
)

test_loss, test_acc = model.evaluate(val_generator, steps=val_generator.samples // batch_size)
print("Test Accuracy:", test_acc)

# Predict labels for the test set
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Convert true labels to one-hot encoded format
y_true = tf.keras.utils.to_categorical(test_generator.labels, num_classes=len(label_to_index))
y_true_classes = np.argmax(y_true, axis=1)

# Generate classification report
print("Classification Report:")
print(classification_report(y_true_classes, y_pred_classes, target_names=label_to_index.keys()))

# Generate confusion matrix
conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)
print("Confusion Matrix:")
print(conf_matrix)

